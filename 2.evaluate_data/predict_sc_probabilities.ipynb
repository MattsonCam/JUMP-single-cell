{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "jukit_cell_id": "EzUt4Twg5N"
   },
   "source": [
    "# Predict Single Cell Probabilities\n",
    "In this part of the analysis, the single cell probabilities are predicted and stored in a parquet file.\n",
    "The index of this file is used to reference the single cell index in the corresponding parquet files, where the \"Metadata_plate\" column refers to the plate name corresponding to the reference file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jukit_cell_id": "OvtZf6TNyh"
   },
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-12T02:20:03.575239Z",
     "iopub.status.busy": "2023-08-12T02:20:03.575085Z",
     "iopub.status.idle": "2023-08-12T02:20:03.892327Z",
     "shell.execute_reply": "2023-08-12T02:20:03.891959Z"
    },
    "jukit_cell_id": "QuNHIYPoOl"
   },
   "outputs": [],
   "source": [
    "import gzip\n",
    "import io\n",
    "import pathlib\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "from joblib import load"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jukit_cell_id": "nerqgPEMEZ"
   },
   "source": [
    "## Find the path of the git directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-12T02:20:03.894742Z",
     "iopub.status.busy": "2023-08-12T02:20:03.894552Z",
     "iopub.status.idle": "2023-08-12T02:20:03.897618Z",
     "shell.execute_reply": "2023-08-12T02:20:03.897360Z"
    },
    "jukit_cell_id": "w2uAipPz4U"
   },
   "outputs": [],
   "source": [
    "# Get the current working directory\n",
    "cwd = pathlib.Path.cwd()\n",
    "\n",
    "if (cwd / \".git\").is_dir():\n",
    "    root_dir = cwd\n",
    "\n",
    "else:\n",
    "    root_dir = None\n",
    "    for parent in cwd.parents:\n",
    "        if (parent / \".git\").is_dir():\n",
    "            root_dir = parent\n",
    "            break\n",
    "\n",
    "# Check if a Git root directory was found\n",
    "if root_dir is None:\n",
    "    raise FileNotFoundError(\"No Git root directory found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jukit_cell_id": "qykovcaCfD"
   },
   "source": [
    "## Load data code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-12T02:20:03.899420Z",
     "iopub.status.busy": "2023-08-12T02:20:03.899220Z",
     "iopub.status.idle": "2023-08-12T02:20:03.901576Z",
     "shell.execute_reply": "2023-08-12T02:20:03.901323Z"
    },
    "jukit_cell_id": "IRcafZmPA0"
   },
   "outputs": [],
   "source": [
    "def load_joblib_from_url(url):\n",
    "    \"\"\"\n",
    "    Retirieve joblib or gzip csv file from url\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    url : url\n",
    "        The raw url of the file\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    obj : any Python object\n",
    "    \"\"\"\n",
    "\n",
    "    response = requests.get(url)\n",
    "    file_object = io.BytesIO(response.content)\n",
    "\n",
    "    if \".csv\" in url:\n",
    "        file_object = gzip.GzipFile(fileobj=file_object)\n",
    "        obj = pd.read_csv(file_object)\n",
    "\n",
    "    elif \".joblib\" in url:\n",
    "        obj = load(file_object)\n",
    "\n",
    "    return obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-12T02:20:03.903315Z",
     "iopub.status.busy": "2023-08-12T02:20:03.903116Z",
     "iopub.status.idle": "2023-08-12T02:20:03.905408Z",
     "shell.execute_reply": "2023-08-12T02:20:03.905158Z"
    },
    "jukit_cell_id": "j39BPjtiy5"
   },
   "outputs": [],
   "source": [
    "# The paths of the models\n",
    "base_model_path = \"https://github.com/WayScience/phenotypic_profiling_model/raw/main/2.train_model/models/multi_class_models\"\n",
    "model_paths = [f\"{base_model_path}/final__CP.joblib\", f\"{base_model_path}/shuffled_baseline__CP.joblib\"]\n",
    "\n",
    "# The path of the data used for the data used for inferencing in the phenotypic_profiling_model repo\n",
    "data_path = \"https://github.com/WayScience/phenotypic_profiling_model/raw/main/0.download_data/data/labeled_data.csv.gz\"\n",
    "\n",
    "# Path to the drive\n",
    "drive_path = f\"{root_dir}/big_drive\"\n",
    "\n",
    "# The predicted probabilities of the models on each cell from each plate\n",
    "output_proba_path = f\"{drive_path}/probability_sc_data\"\n",
    "\n",
    "# The path of the normalized sc data\n",
    "norm_data_path = pathlib.Path(f\"{drive_path}/normalized_sc_data\")\n",
    "\n",
    "# The path of the model's predicted probabilities\n",
    "pathlib.Path(f\"{output_proba_path}\").mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jukit_cell_id": "O8TS0wIrg6"
   },
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-12T02:20:03.907204Z",
     "iopub.status.busy": "2023-08-12T02:20:03.906923Z",
     "iopub.status.idle": "2023-08-12T02:20:07.697133Z",
     "shell.execute_reply": "2023-08-12T02:20:07.696715Z"
    },
    "jukit_cell_id": "8nQDGIVMZe"
   },
   "outputs": [],
   "source": [
    "# Store the models as a dictionary\n",
    "models = {model_path.split(\"/\")[-1].split(\"__\")[0]: load_joblib_from_url(model_path) for model_path in model_paths}\n",
    "\n",
    "# Original dataset used to select features for models\n",
    "data = load_joblib_from_url(data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jukit_cell_id": "Od3Z4Ripve"
   },
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-12T02:20:07.700023Z",
     "iopub.status.busy": "2023-08-12T02:20:07.699867Z",
     "iopub.status.idle": "2023-08-12T03:01:41.175477Z",
     "shell.execute_reply": "2023-08-12T03:01:41.174251Z"
    },
    "jukit_cell_id": "LxCzoc8wlD"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total cell count is 20959860\n"
     ]
    }
   ],
   "source": [
    "# Extract CP features from all columns depending on desired dataset\n",
    "feature_cols = [col for col in data.columns if \"CP__\" in col]\n",
    "feature_cols = [string.replace(\"CP_\", \"Nuclei\") if \"CP\" in string else string for string in feature_cols]\n",
    "feature_cols = pd.Index(feature_cols)\n",
    "model_preds = []\n",
    "cell_count = 0\n",
    "\n",
    "# Iterate through the normalized plate paths\n",
    "for plate in norm_data_path.iterdir():\n",
    "\n",
    "    # Load the dataframe for the plate\n",
    "    df = pd.read_parquet(plate)\n",
    "\n",
    "    # Retrieve the name of the parquet file\n",
    "    parquet_name = plate.name.split('_')[0]\n",
    "\n",
    "    # Find columns that were in the original dataset used for inferencing, but not in the new dataset\n",
    "    new_df_cols = feature_cols.difference(df.columns).tolist()\n",
    "\n",
    "    # Set the columns found above to all zero values\n",
    "    df[new_df_cols] = 0\n",
    "\n",
    "    # Get the wells of the plate\n",
    "    well = df[\"Metadata_Well\"]\n",
    "\n",
    "    # Get the plate of the dataframe\n",
    "    plate_name = df[\"Metadata_Plate\"].iloc[0]\n",
    "\n",
    "    # Order the column based on order of columns used to train model\n",
    "    df = df[feature_cols]\n",
    "\n",
    "    # Convert dataframe to matrix\n",
    "    df_mat = df.values\n",
    "\n",
    "    # Calculate the number of cells\n",
    "    cell_count += df_mat.shape[0]\n",
    "\n",
    "    # Create predictions for each model\n",
    "    for model_type, model in models.items():\n",
    "\n",
    "        # Find the probabilities\n",
    "        preds = model.predict_proba(df_mat)\n",
    "\n",
    "        # Store the predictions using the models classes\n",
    "        predsdf = pd.DataFrame(preds, columns=model.classes_)\n",
    "\n",
    "        # Store the type of model\n",
    "        predsdf[\"Metadata_model_type\"] = model_type\n",
    "\n",
    "        # Store the well data\n",
    "        predsdf[\"Metadata_Well\"] = well\n",
    "\n",
    "        # Store the plate data\n",
    "        predsdf[\"Metadata_plate\"] = plate_name\n",
    "\n",
    "        # Store the prediction dataframes\n",
    "        model_preds.append(predsdf)\n",
    "\n",
    "# Concatenate the model predictions\n",
    "model_preds = pd.concat(model_preds)\n",
    "\n",
    "# Save predictions\n",
    "model_preds.to_parquet(f\"{output_proba_path}/model_probabilities.parquet\", index=True)\n",
    "\n",
    "print(f\"The total cell count is {cell_count}\")"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
